---
title: "Homework3"
output: html_document
---

#########################
## EXERCISE 1
#########################

1. Use the auto data set from JWHT problem 3.9 on page 122.
     
(a) The origin variable is categorical, where 1=US, 2=Europe and 3=Japan. Type the following command to make it a factor variable and assign meaningful labels:

```{r}
auto = read.table("http://www-bcf.usc.edu/~gareth/ISL/Auto.data", header=T, na.strings="?")
     auto$origin = factor(auto$origin, 1:3, c("US", "Europe", "Japan"))
```

(b) Regress mpg on origin, weight and year. Examine the diagnostic plots and comment on which assumptions of the linear model, if any, are violated.

```{r auto}
fit = lm(mpg ~ origin + weight + year, auto)
plot(fit)
```

<!--
- It looks like the residuals form a U shape and is not a snowstorm random pattern. The Q-Q model shows a deviation from the line. This is not a linear model, so should not have tried to fit a linear regresion on the data.
-->

(c) Regress log(mpg) on origin, log(weight), year and year squared. Examine the diagnostic plots and the summary. For you to think about but not turn in: why would year have this effect for year?

```{r}
fit2 = lm(log(mpg) ~ origin + log(weight) + year + I(year^2), auto)
summary(fit2)
plot(fit2)
```

<!--
Call:
lm(formula = log(mpg) ~ origin + log(weight) + year + I(year^2), 
    data = auto)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.37408 -0.06782  0.00899  0.06903  0.35766 

Coefficients:
               Estimate Std. Error t value Pr(>|t|)    
(Intercept)  18.4693014  2.6833895   6.883 2.34e-11 ***
originEurope  0.0668291  0.0176293   3.791 0.000174 ***
originJapan   0.0319711  0.0179382   1.782 0.075477 .  
log(weight)  -0.8750305  0.0270390 -32.362  < 2e-16 ***
year         -0.2559684  0.0712094  -3.595 0.000366 ***
I(year^2)     0.0019051  0.0004687   4.065 5.81e-05 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.1136 on 391 degrees of freedom
Multiple R-squared:  0.8898,	Adjusted R-squared:  0.8884 
F-statistic: 631.7 on 5 and 391 DF,  p-value: < 2.2e-16
-->

<!--
- Check the null hypothesis, that the year and the quadratic slope I(year^2) are both equal to zero. So the null hypothesis can be rejected since both values are < 0.05. year is 0.000366 and I(year^2) is 5.81e-05.
-->

(d) Describe the effect of year on log(mpg), i.e., is it U-shaped, inverted-U shaped, or linear? If it is nonlinear, where is the minimum or maximum. Draw a graph showing the effect.

```{r}
fit3 = lm(year ~ log(mpg), auto)
summary(fit3)
plot(fit3)
```

<!--
Call:
lm(formula = year ~ log(mpg), data = auto)

Residuals:
    Min      1Q  Median      3Q     Max 
-7.8418 -2.0185  0.3031  2.5042  6.4684 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  56.5499     1.3901   40.68   <2e-16 ***
log(mpg)      6.2700     0.4456   14.07   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 3.015 on 395 degrees of freedom
Multiple R-squared:  0.3339,	Adjusted R-squared:  0.3322 
F-statistic:   198 on 1 and 395 DF,  p-value: < 2.2e-16
-->

<!--
- The data looks linear, but there is a slight U-shape
- Check the null hypothesis, that the log slope log(mpg) is equal to zero. The Null hypothesis can be rejected since the p values is < 0.05. the log(mpg) is 2e-16.
-->

(e) What does the coefficient for log(weight) tell you?
```{r}
coef(fit2)
```

<!--
 (Intercept) originEurope  originJapan  log(weight)         year    I(year^2) 
18.469301378  0.066829060  0.031971087 -0.875030521 -0.255968375  0.001905147 
-->

<!--
- This information can be seen on the summary in (c) as well, showing that there is a negative relationship. So there is a negative effect where the heavier a vehicle is, mpg goes down -0.875030521. Weight of a vehicle has a negative impact on vehicle mpg.
-->

(f) You should see that there are two dummy variables for the origin variable. If origin were dropped from the model (i.e., the two dummies were set equal to 0), by how much would RSS increase?
<!--
Dummy variables page 82 and 83 in packet. This is how we use nominal variables in a regression. They are called dummy (indicator) variables. Example was the quality case where AM and PM were compared and a variable (am) was assigned that took a value of 1 for morning shift and 0 for afternoon.

Here origin was set to US, Europe, and Japan
-->

```{r}
fit4 = lm(log(mpg) ~ as.factor(origin) + log(weight) + year + I(year^2), auto)
summary(fit4)
drop1(fit4, test="F")
```
<!--
Call:
lm(formula = log(mpg) ~ as.factor(origin) + log(weight) + year + 
    I(year^2), data = auto)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.37408 -0.06782  0.00899  0.06903  0.35766 

Coefficients:
                          Estimate Std. Error t value Pr(>|t|)    
(Intercept)             18.4693014  2.6833895   6.883 2.34e-11 ***
as.factor(origin)Europe  0.0668291  0.0176293   3.791 0.000174 ***
as.factor(origin)Japan   0.0319711  0.0179382   1.782 0.075477 .  
log(weight)             -0.8750305  0.0270390 -32.362  < 2e-16 ***
year                    -0.2559684  0.0712094  -3.595 0.000366 ***
I(year^2)                0.0019051  0.0004687   4.065 5.81e-05 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.1136 on 391 degrees of freedom
Multiple R-squared:  0.8898,	Adjusted R-squared:  0.8884 
F-statistic: 631.7 on 5 and 391 DF,  p-value: < 2.2e-16

Single term deletions

Model:
log(mpg) ~ as.factor(origin) + log(weight) + year + I(year^2)
                  Df Sum of Sq     RSS     AIC   F value    Pr(>F)    
<none>                          5.0449 -1721.1                        
as.factor(origin)  2    0.1857  5.2306 -1710.8    7.1968 0.0008523 ***
log(weight)        1   13.5126 18.5574 -1206.0 1047.2853 < 2.2e-16 ***
year               1    0.1667  5.2116 -1710.2   12.9211 0.0003664 ***
I(year^2)          1    0.2132  5.2580 -1706.7   16.5224 5.814e-05 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
-->

(g) Can you reject the null hypothesis that both origin dummies are 0, so that none of the origin levels have different effects?
<!--
yes, we can reject the null hypothesis, the drop1 shows that the p-value is again the p-value is 0.0008523 for categorical origin which is < 0.05.
-->

(h) Interpret the effect of origin on log(mpg). Which origin has the best gas milage? Worst? Rank them in order of gas milage from least to greatest.
<!--
Data from above summary

as.factor(origin)Europe  0.0668291  0.0176293   3.791 0.000174 ***
as.factor(origin)Japan   0.0319711  0.0179382   1.782 0.075477 .  
-->

<!--
- The interpretation of this data is that Europe is 0.0668291 better in mpg than the US
- That Japan is 0.0319711 better in mpg than the US
-->

(i) For the test in question 1g, give the details of how the test statistic and P-value were computed.
<!--
- below is the data from the drop1, note that the test statistic F value is 7.1968 for origin
- note that the p value is 0.0008523

Single term deletions

Model:
log(mpg) ~ as.factor(origin) + log(weight) + year + I(year^2)
                  Df Sum of Sq     RSS     AIC   F value    Pr(>F)    
<none>                          5.0449 -1721.1                        
as.factor(origin)  2    0.1857  5.2306 -1710.8    7.1968 0.0008523 ***
log(weight)        1   13.5126 18.5574 -1206.0 1047.2853 < 2.2e-16 ***
year               1    0.1667  5.2116 -1710.2   12.9211 0.0003664 ***
I(year^2)          1    0.2132  5.2580 -1706.7   16.5224 5.814e-05 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
-->

Look at deviance

```{r}
fit4 = lm(log(mpg) ~ log(weight) + year + I(year^2), auto)
deviance(fit4)
deviance(fit3)
deviance(fit4) - deviance(fit3)
```
<!--
[1] 5.230573  - deviance(fit4) (fit4 = log(mpg) ~ log(weight) + year + I(year^2), auto)
[1] 5.04486  - deviance(fit3) (fit3 = lm(mpg ~ as.factor(origin) + weight + year, auto)
[1] 0.1857133  - deviance(fit4) - deviance(fit3)

- the deviance is what is the amount unexplained by the model
- fit4 accounts for less so there is more that is unexplained
- fit3 accounts for the categorical variable which includes origin, so there is less unexplained by that model
- if you take the difference of the deviance of fit4 and fit3 you get 0.1857133, this is the same as what the drop1 calculates. So this is how drop1 calculates the Sum of Sq, or what is explained by the model.
-->

The F statistic
```{r}
anova(fit3)
```
<!--
Analysis of Variance Table

Response: log(mpg)
                   Df  Sum Sq Mean Sq  F value    Pr(>F)    
as.factor(origin)   2 15.1819  7.5910  588.335 < 2.2e-16 ***
log(weight)         1 19.9156 19.9156 1543.552 < 2.2e-16 ***
year                1  5.4437  5.4437  421.912 < 2.2e-16 ***
I(year^2)           1  0.2132  0.2132   16.522 5.814e-05 ***
Residuals         391  5.0449  0.0129                       
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
-->

```{r}
(deviance(fit4)-deviance(fit3))/2/0.0129
```
<!--
[1] 7.198191
-->

<!--
from the drop1 table this matches the 7.1968 (snippet from drop 1 table below)

                  Df Sum of Sq     RSS     AIC   F value    Pr(>F)    
<none>                          5.0449 -1721.1                        
as.factor(origin)  2    0.1857  5.2306 -1710.8    7.1968 0.0008523 ***
-->

The P value
```{r}
1-pf((deviance(fit4)-deviance(fit3))/2/0.0129, 2, 391)
```
<!--
[1] 0.00085122
-->

<!--
- this is using the anova df 2 (origin) and df 391 (residual) to calculate the p-value, which is 0.00085122, which is close to what R calculated which was 0.0008523.
-->

(j) After controlling for the other vairables, what is the difference in log(mpg) between Japan and Europe, on the average?
<!--
not quite sure what to do here, have to think...
-->

(k) Is there a significant difference between the log gas milage for US and Japan after controlling for the other variables?
<!--
not quite sure what to do here, have to think...
-->


#########################
## EXERCISE 2
#########################

2. A marketing research consultant evaluated the effects of the fee schedule, scope of work, and type of supervisory control on the quality of work performed under contract by independent marketing research agencies. The quality of work performed was measured by an index taking into account several characteristics of quality. Four agencies were chosen for each factor level combination and the quality of their work evaluated. Copy and paste the data below into R.

```{r}
   mrcontract = expand.grid(agency=LETTERS[1:4], sup=c("local","travel"),
     scope=c("in-house", "subcontract"), fee=c("high","med","low"))
   mrcontract$quality=c(124.3,120.6,120.7,122.6,112.7,110.2,113.5,108.6,115.1,
     119.9,115.4,117.3,88.2,96,96.4,90.1,119.3,118.9,125.3,121.4,113.6,109.1,108.9,
     112.3,117.2,114.4,113.4,120,92.7,91.1,90.7,87.9,90.9,95.3,88.8,92,78.6,80.6,
     83.5,77.1,89.9,83,86.5,82.7,58.6,63.5,59.8,62.3)
```

(a) Regress quality on agency, fee and an interaction between sup and scope. State the estimated regression equation and use drop1 to test which terms are signfiicant.
(b) Are there differences in quality between the agencies? To receive full credit state the null and alternative hypotheses, find the P value, state you decision (reject or not), and summarize your conclusion.
(c) Are there differences in quality between the fee values? To receive full credit state the null and alternative hypotheses, find the P value, state you decision (reject or not), and summarize your conclusion.
(d) What does the coefficient for feemed tell you? Test whether it is different from 0 and discuss what the results of this tell you from a managerial perspective.
(e) Is the interaction between sup and scope significant? To receive full credit state the null and alternative hypotheses, find the P value, and state you decision (reject or not).
(f) Construct and interaction plot for sup and scope. Write one sentence summariz- ing what the interaction plot tells you.


#########################
## EXERCISE 3
#########################
3. Use the data set part.csv, available from canvas. This question investigates how par- ticipation in a social media contest about a brand affect future spending on the brand. A brand sponsored a social media contest. Customers in the company’s database were invited to write about their relationship with the company on a social media forum. Those who participated by writing at least one word on the forum received a reward worth approximately $1, and the dummy variable tx indicates whether or not a cus- tomer participated. In total, 7089 customers participated, and there is a matched control group of 7089 consistent of customers who did not participate, but had similar purchase activities prior to the contest. The total sample size is thus 2×7089 = 14, 178. The variable y records the amount spent by each customer in the week following the contest. The variable x gives the amount spent per week prior to the contest and will be used as a control variable to account for differences in customer loyalty. Finally, the wc variable gives the word count of the entries, where wc = 0 for all who did not participate. Word count measures cognitive elaboration. Note that tx = (wc > 0).

(a) Model 1: regress log(y + 1) on log(x + 1) and tx. Give the output.
(b) Model 2: regress log(y + 1) on log(x + 1), tx and log(wc + 1). Give the output.
(c) Use the following notation in answering the questions: log(y+1)=β0 +β1log(x+1)+β2tx+β3log(wc+1)+e,
where β3 is constrained to be 0 in Model 1 and log is the natural log. Based on Model 1, does participation have a significant effect on future spending? Explain. Note: to receive full credit you should state null and alternative hypotheses and do something to determine whether H0 can be rejected at the 5% level.
(d) Using Model 1, post-period spending is how many times greater for those who participate than for those who do not? Note that this question asks about spending and not log spending. Another way to ask this question is, suppose there are two people with identical pre-period spending, but one participates and the other does not. If y1 is the post-contest spending of a participant, and y0 is the post-contest spending of a non-participant, how many times greater is (y1 + 1) than (y0 + 1)?
(e) Is (y + 1) proportional to (x + 1), i.e., is spending in the week after the contest proportional to pre-contest spending? How do you know? Note: to receive full credit, state a null and alternative hypothesis and do something to determine whether or not H0 can be rejected.
(f) Why is the magnitude of the tx variable so different in Model 2 (0.050) than in Model 1 (0.244)?
(g) Now consider Model 2. How do the results from Model 2 change your conclu- sions about how participation affects future spending. I am looking for you to summarize the key learnings from Model 2 succinctly.
(h) Generate the normal probability plot for Model 2. What specifically does the plot tell you, and how does it (i.e., what the plot tells you) affect the conclusions you have drawn from the previous parts.
(i) What do the results of this analysis suggest the company should do in the future when designing social media contests?